# -*- coding: utf-8 -*-
"""MusicGenreClassification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RYj5BDtXwIoCUAsnXncAhN2P1Wyz9mqX

# **Music genre Classification**
---
Pablo Gavilán Estepa (105550)

Álvaro San Emeterio Valdes

## Task 1
** Design a k-NN classifier (k=5) for all ten genres using only the following four features; spectral rolloff mean, mfcc 1 mean, spectral centroid mean and tempo. Evaluate the performance of the classification model.**
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.impute import SimpleImputer
import seaborn as sns
import matplotlib.pyplot as plt

# Loading data
data = pd.read_csv('/content/drive/MyDrive/Music Genre Classification/GenreClassData_30s.txt', sep = "\t")

# Feature selection and normalization
# Imputation of missing values
imputer = SimpleImputer(strategy='mean')
features = ['spectral_rolloff_mean', 'mfcc_1_mean', 'spectral_centroid_mean', 'tempo']
data[features] = imputer.fit_transform(data[features])

# Preparation of features and labels
X = data[features]
y = data['GenreID']

# Data normalization
X_normalized = StandardScaler().fit_transform(X)

# Data splitting
X_train, X_test, y_train, y_test = train_test_split(X_normalized, y, test_size=0.2, random_state=42)

# k-NN model creation and training
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train, y_train)

# Cross-validation with 10 folds
cv_scores = cross_val_score(knn, X_normalized, y, cv=10)
print("Accuracy scores for each fold:", cv_scores)
print("Mean CV accuracy:", np.mean(cv_scores))

# Prediction and evaluation
y_pred = knn.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print('Accuracy without cross_val:', accuracy)

'''CONFUSION MATRIX AND CLASSIFICATION REPORT'''

# Confusion Matrix
genre_names = data[['GenreID', 'Genre']].drop_duplicates().sort_values(by='GenreID')['Genre'].tolist()
conf_mat = confusion_matrix(y_test, y_pred)
sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues', xticklabels=genre_names, yticklabels=genre_names)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

# Clasification Report
print(classification_report(y_test, y_pred, target_names=genre_names))

# Accuracy
accuracy = accuracy_score(y_test, y_pred)
print('Accuracy:', accuracy)

"""## TASK 2:
**For each of the four features; spectral rolloff mean, mfcc 1 mean, spec- tral centroid mean and tempo, compare the feature distribution for the four classes; pop, disco, metal and classical. Analyze how the feature distribution relates to the performance of your classifier.**
"""

# Filtrar el dataset para incluir solo las clases seleccionadas
selected_genres = ['pop', 'disco', 'metal', 'classical']
filtered_data = data[data['Genre'].isin(selected_genres)]

# Lista de las características a analizar
features = ['spectral_rolloff_mean', 'mfcc_1_mean', 'spectral_centroid_mean', 'tempo']

# Graficar las distribuciones de las características para los géneros seleccionados
fig, axes = plt.subplots(2, 2, figsize=(16, 12))
axes = axes.flatten()

for i, feature in enumerate(features):
    sns.histplot(data=filtered_data, x=feature, hue='Genre', kde=True, ax=axes[i])
    axes[i].set_title(f'Distribution of {feature} by Genre')
    axes[i].set_xlabel(feature)
    axes[i].set_ylabel('Density')

plt.tight_layout()
plt.show()

"""## Ejercicio 3:  Design a k-NN classifier (k=5) for all ten genres using only four features with at least three features being; spectral rolloff mean, mfcc 1 mean, spectral centroid mean or tempo. Motivate why you selected the particular four features."""

from sklearn.feature_selection import SelectKBest, f_classif
# Selecting three required features
features = ['spectral_rolloff_mean', 'mfcc_1_mean', 'spectral_centroid_mean', 'tempo']

# We need to choose a fourth feature. Let's analyze the correlations to make an informed decision.
correlation_matrix = data[features].corr()

# Plotting the correlation matrix
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Matrix of Features')
plt.show()

# Selecting columns
columns_to_drop = ['GenreID', 'Genre', 'Type', 'Track ID', 'File']
features = [col for col in data.columns if col not in columns_to_drop]

# Making sure there are no NaN or infinite values in the columns
data[features] = data[features].replace([np.inf, -np.inf], np.nan)
data[features] = data[features].fillna(data[features].mean())

X = data[features].values
y = data['GenreID'].values

# Fitting SelectKBest
selector = SelectKBest(score_func=f_classif, k='all')
selector.fit(X, y)

# Now we should get a score for each feature
scores30s = selector.scores_

# Creating DataFrame
feature_scores = pd.DataFrame({'Feature': features, 'Score': scores30s})

# Sorting and displaying results
feature_scores = feature_scores.sort_values(by='Score', ascending=False)

# Selecting the top 10 features
top_10_features = feature_scores.head(10)

# Printing the top 10 features
print(top_10_features)

# Visualizing the top 5 features
plt.figure(figsize=(10, 6))
plt.barh(top_10_features['Feature'], top_10_features['Score'])
plt.xlabel('Score')
plt.ylabel('Features')
plt.title('Top 10 most important features (SelectKBest)')
plt.gca().invert_yaxis()  # Invert the y-axis so that the most important feature is at the top
plt.show()

correlation_matrix = data[top_10_features['Feature']].corr()

# Plotting the correlation matrix
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Matrix of top 10 Features')
plt.show()

# Feature selection and normalization
# Imputation of missing values
imputer = SimpleImputer(strategy='mean')
features = ['spectral_centroid_mean', 'mfcc_1_mean', 'rmse_var', 'tempo']
data[features] = imputer.fit_transform(data[features])

# Preparation of features and labels
X = data[features]
y = data['GenreID']

# Data Normalization
X_normalized = StandardScaler().fit_transform(X)

# Data division
X_train, X_test, y_train, y_test = train_test_split(X_normalized, y, test_size=0.2, random_state=42)

# Setting and fitting the k-NN model
knn3 = KNeighborsClassifier(n_neighbors=5)
knn3.fit(X_train, y_train)

# Prediction and evaluation
y_pred = knn3.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print('Accuracy without cross_val:', accuracy)

# Confusion Matrix
genre_names = data[['GenreID', 'Genre']].drop_duplicates().sort_values(by='GenreID')['Genre'].tolist()
conf_mat = confusion_matrix(y_test, y_pred)
sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues', xticklabels=genre_names, yticklabels=genre_names)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

# Classification Report
print(classification_report(y_test, y_pred, target_names=genre_names))

# Accuracy
accuracy = accuracy_score(y_test, y_pred)
print('Accuracy:', accuracy)

"""# Task4:
**Design a classifier for all ten genres that classifies the audio tracks, each represented by a Track ID. You are allowed to use any classifier, as many features as you like and all of the available data sets GenreClassData 5s.txt, GenreClassData 10s.txt, and GenreClassData 30s.txt as input data.**

"""

data30s = pd.read_csv('/content/drive/MyDrive/Music Genre Classification/GenreClassData_30s.txt', sep = "\t")
data5s = pd.read_csv('/content/drive/MyDrive/Music Genre Classification/GenreClassData_5s.txt', sep = "\t")
data10s = pd.read_csv('/content/drive/MyDrive/Music Genre Classification/GenreClassData_10s.txt', sep = "\t")

"""#Feature Selection

In this part of the code, we're going to iterate through each type of feature, seeing which one can help us the most. We'll do this using a 'score'.

The "score" in this context refers to the ANOVA F-value score of each feature. The higher the score, the more likely the feature has a statistically significant relationship with the target variable you're trying to predict.

# MFCC's
"""

data = pd.read_csv('/content/drive/MyDrive/Music Genre Classification/GenreClassData_5s.txt', sep="\t")
from sklearn.feature_selection import SelectKBest, f_classif
# Selecting MFCC columns
mfcc_features = [col for col in data.columns if 'mfcc' in col]

# Making sure there are no NaN or infinite values in the MFCC columns
data[mfcc_features] = data[mfcc_features].replace([np.inf, -np.inf], np.nan)
data[mfcc_features] = data[mfcc_features].fillna(data[mfcc_features].mean())

X = data[mfcc_features].values
y = data['GenreID'].values

# Fitting SelectKBest
selector = SelectKBest(score_func=f_classif, k='all')
selector.fit(X, y)

# Now we should get a score for each MFCC feature
scores = selector.scores_

# Creating DataFrame
feature_scores = pd.DataFrame({'Feature': mfcc_features, 'Score': scores})

# Sorting and displaying results
feature_scores = feature_scores.sort_values(by='Score', ascending=False)
print(feature_scores)

"""#Chroma's"""

# Select Chroma columns
chroma_features = [col for col in data.columns if 'chroma' in col]

# Ensure there are no NaN or infinite values in the Chroma columns
data[chroma_features] = data[chroma_features].replace([np.inf, -np.inf], np.nan)
data[chroma_features] = data[chroma_features].fillna(data[chroma_features].mean())

X = data[chroma_features].values
y = data['GenreID'].values

# Fit SelectKBest
selector = SelectKBest(score_func=f_classif, k='all')
selector.fit(X, y)

# Now we should get a score for each Chroma feature
scores = selector.scores_

# Create DataFrame
feature_scores = pd.DataFrame({'Feature': chroma_features, 'Score': scores})

# Sort and display results
feature_scores = feature_scores.sort_values(by='Score', ascending=False)
print(feature_scores)

"""#Spectral's

"""

# Select Spectral columns
spectral_features = [col for col in data.columns if 'spectral' in col]

# Ensure there are no NaN or infinite values in the Spectral columns
data[spectral_features] = data[spectral_features].replace([np.inf, -np.inf], np.nan)
data[spectral_features] = data[spectral_features].fillna(data[spectral_features].mean())

X = data[spectral_features].values
y = data['GenreID'].values

# Fit SelectKBest
selector = SelectKBest(score_func=f_classif, k='all')
selector.fit(X, y)

# Now we should get a score for each Spectral feature
scores = selector.scores_

# Create DataFrame
feature_scores = pd.DataFrame({'Feature': spectral_features, 'Score': scores})

# Sort and display results
feature_scores = feature_scores.sort_values(by='Score', ascending=False)
print(feature_scores)

"""# Rest of the Features"""

# Select Spectral columns
zero_or_rmse_features = [col for col in data.columns if col.startswith('zero') or col.startswith('rmse') or col.startswith('tempo')]

# Ensure there are no NaN or infinite values in the Spectral columns
data[zero_or_rmse_features] = data[zero_or_rmse_features].replace([np.inf, -np.inf], np.nan)
data[zero_or_rmse_features] = data[zero_or_rmse_features].fillna(data[zero_or_rmse_features].mean())

X = data[zero_or_rmse_features].values
y = data['GenreID'].values

# Fit SelectKBest
selector = SelectKBest(score_func=f_classif, k='all')
selector.fit(X, y)

# Now we should get a score for each Spectral feature
scores = selector.scores_

# Create DataFrame
feature_scores = pd.DataFrame({'Feature': zero_or_rmse_features, 'Score': scores})

# Sort and display results
feature_scores = feature_scores.sort_values(by='Score', ascending=False)
print(feature_scores)

"""#Review of all features"""

# Select columns
columns_to_drop = ['GenreID', 'Genre', 'Type', 'Track ID', 'File']
features = [col for col in data.columns if col not in columns_to_drop]

# Ensure there are no NaN or infinite values in the columns
data[features] = data[features].replace([np.inf, -np.inf], np.nan)
data[features] = data[features].fillna(data[features].mean())

X = data[features].values
y = data['GenreID'].values

# Fit SelectKBest
selector = SelectKBest(score_func=f_classif, k='all')
selector.fit(X, y)

# Now we should get a score for each feature
scores30s = selector.scores_

# Create DataFrame
feature_scores_30s = pd.DataFrame({'Feature': features, 'Score': scores30s})

# Sort and display results
feature_scores_30s = feature_scores_30s.sort_values(by='Score', ascending=False)

# Select the top 10 features
top_10_features_30s = feature_scores_30s.head(10)

# Print the top 10 features
print(top_10_features_30s)

# Visualize the top 10 features
plt.figure(figsize=(10, 6))
plt.barh(top_10_features_30s['Feature'], top_10_features_30s['Score'])
plt.xlabel('Score')
plt.ylabel('Feature')
plt.title('Top 10 features (SelectKBest)')
plt.gca().invert_yaxis()  # Invert the y-axis so that the most important feature is at the top
plt.show()

data = pd.read_csv('/content/drive/MyDrive/Music Genre Classification/GenreClassData_5s.txt', sep = "\t")

"""#CNN

"""

# IMPORT LIBRARIES

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.model_selection import KFold
from sklearn.preprocessing import LabelEncoder
from keras.models import Sequential
from keras.layers import Dense, Dropout

data = pd.read_csv('/content/drive/MyDrive/Music Genre Classification/GenreClassData_5s.txt', sep="\t")

features = ['mfcc_1_mean',
            'chroma_stft_2_mean',
            'spectral_bandwidth_mean', 'spectral_rolloff_mean', 'spectral_centroid_mean',
            'rmse_var', 'rmse_mean',
            'zero_cross_rate_std', 'zero_cross_rate_mean',
            'tempo' ]
X = data[features]

#Apply scaling to numerical features
from sklearn.preprocessing import StandardScaler
fit = StandardScaler()
X = fit.fit_transform(X.astype(float))

#Split the data into training and test sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)

#Building the model

from keras.models import Sequential
from keras.layers import Dense, Dropout
import pandas as pd
import matplotlib.pyplot as plt

def trainModel(model, epochs, optimizer):
    batch_size = 128
    #callback = myCallback()
    model.compile(optimizer=optimizer,
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])
    return model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=batch_size)

def plotValidate(history):
    print("Validation Accuracy", max(history.history["val_accuracy"]))
    pd.DataFrame(history.history).plot(figsize=(12, 6))
    plt.show()

model = Sequential([
    Dense(512, activation='relu', input_shape=(X_train.shape[1],)),
    Dropout(0.4),
    Dense(256, activation='relu'),
    Dropout(0.4),
    Dense(128, activation='relu'),
    Dropout(0.4),
    Dense(64, activation='relu'),
    Dropout(0.4),
    Dense(10, activation='softmax'),
])
print(model.summary())
model_history = trainModel(model=model, epochs=600, optimizer='adam')
plotValidate(model_history)

# Model Evaluation
test_loss, test_acc = model.evaluate(X_test, y_test, batch_size=128)
print("The test Loss is: ", test_loss)
print("The best test Accuracy is: ", test_acc*100)

from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns

# Model predictions on the test set
y_pred = model.predict(X_test)
y_pred_labels = np.argmax(y_pred, axis=1)

# Classification report
print("\nClassification report:")
print(classification_report(y_test, y_pred_labels))

# Visualization of the confusion matrix
# Confusion Matrix
y_pred = model.predict(X_test)
y_pred_labels = np.argmax(y_pred, axis=1)
conf_mat = confusion_matrix(y_test, y_pred_labels)

# Retrieving class labels from the LabelEncoder
from sklearn.preprocessing import LabelEncoder
genre_labels = convertor.classes_

# Visualization of the confusion matrix
plt.figure(figsize=(4, 3))
sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues', xticklabels=genre_labels, yticklabels=genre_labels)
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()